{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/PJWSTK/s14028/engineer/Practical\n"
     ]
    }
   ],
   "source": [
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple, Iterable\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import scipy.io as mat\n",
    "\n",
    "from common import *\n",
    "from augmentation import add_pmap\n",
    "from testing import mse\n",
    "from testing import mae\n",
    "\n",
    "from CNN.CNN_v9 import CNN_v9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "perspective = mat.loadmat(\"mall_dataset/perspective_roi.mat\")[\"pMapN\"]\n",
    "\n",
    "perspective /= np.min(perspective)\n",
    "perspective = np.round(perspective).astype(np.uint8)\n",
    "\n",
    "train, test = data_sets()\n",
    "image_tensors = train[0], test[0]\n",
    "person_coo_tensors = train[1], test[1]\n",
    "count_matrix = train[2], test[2]\n",
    "\n",
    "image_train, image_test = image_tensors\n",
    "person_coo_train, person_coo_test = person_coo_tensors\n",
    "count_train, count_test = count_matrix\n",
    "count_train = count_train.astype(np.uint16)\n",
    "count_test = count_test.astype(np.uint16)\n",
    "\n",
    "image_train = add_pmap(image_train, perspective)\n",
    "image_test = add_pmap(image_test, perspective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN_v9((480, 640, 4), split_into_parts=20)\n",
    "\n",
    "images = cnn._prepare_images(image_test)\n",
    "anwsers = cnn._prepare_anwsers(person_coo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.path.join(\"CNN\", \"CNN_v9\")\n",
    "weights_prefix = os.path.join(working_directory, \"weights\")\n",
    "\n",
    "model_names = [\n",
    "    \"cnn_v9_1e_6_18\",\n",
    "    \"cnn_v9_1e_6_99\"\n",
    "]\n",
    "\n",
    "model_paths = [os.path.join(weights_prefix, name) for name in model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6935 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 704ms/step - loss: 0.6934 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "models = [CNN_v9((480, 640, 4), split_into_parts=20) for i in range(len(model_names))]\n",
    "models = list(zip(model_paths, models))\n",
    "\n",
    "for model_name, model in models:\n",
    "    model.def_model()\n",
    "  \n",
    "    model.model.fit(images[:1], anwsers[:1])\n",
    "    model.model = keras.utils.multi_gpu_model(model.model, gpus=2, cpu_merge=False)\n",
    "    model.model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.model.load_weights(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUSIVE = 1\n",
    "\n",
    "def get_all_possible_cut_of_points() -> np.ndarray:\n",
    "    possible_cut_of_points = np.linspace(0, 1, 11)\n",
    "    return possible_cut_of_points\n",
    "\n",
    "def get_all_possible_lower_and_upper_cut_off_points(cut_off_points: np.ndarray) -> Iterable[Tuple[int, int]]:\n",
    "    return ((cut_off_points[lower], cut_off_points[upper]) for lower, upper in get_lower_and_upper_cut_off_points_indices(cut_off_points))\n",
    "\n",
    "def get_lower_and_upper_cut_off_points_indices(cut_off_points: np.ndarray) -> Iterable[Tuple[int, int]]:\n",
    "    for upper in range(len(cut_off_points)):\n",
    "        for lower in range(upper + INCLUSIVE):\n",
    "            yield (lower, upper)\n",
    "\n",
    "cut_of_points: np.ndarray = get_all_possible_cut_of_points()\n",
    "lower_and_upper_cut_off_points: Iterable[Tuple[int, int]] = get_all_possible_lower_and_upper_cut_off_points(cut_of_points)\n",
    "\n",
    "lower_and_upper_cut_off_points = list(lower_and_upper_cut_off_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITHOUT_FIRST = 1\n",
    "WITHOUT_LAST = -1\n",
    "INCLUSIVE = 1\n",
    "\n",
    "def get_all_possible_cut_of_points() -> np.ndarray:\n",
    "    possible_cut_of_points_with_first_and_last = np.linspace(0, 1, 11)\n",
    "    possible_cut_of_points = possible_cut_of_points_with_first_and_last[WITHOUT_FIRST:WITHOUT_LAST]\n",
    "    return possible_cut_of_points\n",
    "\n",
    "def get_all_possible_lower_and_upper_cut_off_points(cut_off_points: np.ndarray) -> Iterable[Tuple[int, int]]:\n",
    "    return ((cut_off_points[lower], cut_off_points[upper]) for lower, upper in get_lower_and_upper_cut_off_points_indices(cut_off_points))\n",
    "\n",
    "def get_lower_and_upper_cut_off_points_indices(cut_off_points: np.ndarray) -> Iterable[Tuple[int, int]]:\n",
    "    for upper in range(len(cut_off_points)):\n",
    "        for lower in range(upper + INCLUSIVE):\n",
    "            yield (lower, upper)\n",
    "\n",
    "cut_of_points: np.ndarray = get_all_possible_cut_of_points()\n",
    "lower_and_upper_cut_off_points: Iterable[Tuple[int, int]] = get_all_possible_lower_and_upper_cut_off_points(cut_of_points)\n",
    "\n",
    "lower_and_upper_cut_off_points = list(lower_and_upper_cut_off_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_CATEGORY_FILLER = 0\n",
    "MIDDLE_CATEGORY_FILLER = 1 / 2\n",
    "UPPER_CATEGORY_FILLER = 1\n",
    "\n",
    "def count_crowd(probabilities_for_each_image: np.ndarray, cut_off_points: List[Tuple[int, int]]) -> Iterable[np.ndarray]:\n",
    "    for lower, upper in cut_off_points:\n",
    "        yield count_crowd_with_lower_and_upper_cut_off_points(probabilities_for_each_image, lower, upper)\n",
    "\n",
    "def count_crowd_with_lower_and_upper_cut_off_points(probabilities_for_each_image: np.ndarray, lower: float, upper: float) -> np.ndarray:    \n",
    "    crowd_counts: np.ndarray = get_crowd_count_matrix_for_lower_and_upper_cut_off_points(probabilities_for_each_image, lower, upper)\n",
    "    crowd_counts = np.sum(crowd_counts, axis=1)\n",
    "    \n",
    "    return crowd_counts\n",
    "\n",
    "def get_crowd_count_matrix_for_lower_and_upper_cut_off_points(probabilities_for_each_image: np.ndarray, lower: float, upper: float) -> np.ndarray:\n",
    "    crowd_counts: np.ndarray = create_empty_crowd_counts(probabilities_for_each_image)\n",
    "    \n",
    "    middle_and_upper_category_indices = probabilities_for_each_image > lower\n",
    "    upper_category_indices = probabilities_for_each_image > upper\n",
    "    \n",
    "    crowd_counts[middle_and_upper_category_indices] = MIDDLE_CATEGORY_FILLER\n",
    "    crowd_counts[upper_category_indices] = UPPER_CATEGORY_FILLER\n",
    "    \n",
    "    return crowd_counts\n",
    "\n",
    "def create_empty_crowd_counts(probabilities_for_each_image: np.ndarray) -> np.ndarray:\n",
    "    return np.full_like(probabilities_for_each_image, LOWER_CATEGORY_FILLER, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(difference: np.ndarray) -> Dict[str, float]:\n",
    "    statistics = {\n",
    "        \"accuracy\": calculate_accuracy(difference),\n",
    "        \"mse\": mse(difference),\n",
    "        \"mae\": mae(difference)\n",
    "    }\n",
    "    \n",
    "    return statistics\n",
    "\n",
    "def calculate_accuracy(difference: np.ndarray) -> float:\n",
    "    return np.mean(calculate_accuracy_core(difference)) * 100\n",
    "\n",
    "def calculate_accuracy_core(difference: np.ndarray) -> float:\n",
    "    return np.clip(1 - (np.abs(difference) / count_test), 0, 1)\n",
    "\n",
    "cut_off_points = lower_and_upper_cut_off_points\n",
    "\n",
    "crowd_counts_probabilities: List[np.ndarray] = [model.predict_proba(image_test) for _, model in models]\n",
    "crowd_counts_generators: Iterable[Iterable[np.ndarray]] = (count_crowd(probabilities, cut_off_points) for probabilities in crowd_counts_probabilities)\n",
    "\n",
    "counts_differences_generators: Iterable[Iterable[np.ndarray]] = ((crowd_counts - count_test for crowd_counts in generator) for generator in crowd_counts_generators)\n",
    "models_statistics: List[List[Dict[str, float]]] = [[calculate_statistics(difference) for difference in generator] for generator in counts_differences_generators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_keys(statistics: List[Dict[str, float]]) -> Dict[str, List[float]]:\n",
    "    grouped_statistics: Dict[str, List[float]] = create_empty_groups(statistics)\n",
    "    \n",
    "    for substatistics in statistics:\n",
    "        grouped_statistics: Dict[str, List[float]] = append_substatistics_to_groups(substatistics, grouped_statistics)\n",
    "    \n",
    "    return grouped_statistics\n",
    "\n",
    "def create_empty_groups(statistics: List[Dict[str, float]]) -> Dict[str, List[float]]:\n",
    "    any_statistics, *_ = statistics\n",
    "    empty_groups = {key: [] for key in any_statistics}\n",
    "    return empty_groups\n",
    "\n",
    "def append_substatistics_to_groups(substatistics: Dict[str, float], groups: Dict[str, List[float]]) -> Dict[str, List[float]]:\n",
    "    for key, value in substatistics.items():\n",
    "        groups[key].append(value)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "models_statistics: List[Dict[str, List[float]]] = [groupby_keys(statistics) for statistics in models_statistics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(left_frame: pd.DataFrame, right_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.concat([left_frame, right_frame], axis=1, sort=False)\n",
    "\n",
    "models_statistics_frames = [pd.DataFrame(statistics) for statistics in models_statistics]\n",
    "cut_off_points_frame = pd.DataFrame(cut_off_points, columns=[\"lower\", \"upper\"])\n",
    "\n",
    "models_statistics_with_cut_off_points_frames = [concatenate(frame, cut_off_points_frame) for frame in models_statistics_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(path: str):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "statistics_path = os.path.join(working_directory, \"statistics\")\n",
    "make_directory(statistics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, statistics in zip(model_names, models_statistics_with_cut_off_points_frames):\n",
    "    current_statistics_path = os.path.join(statistics_path, f\"{model_name}.csv\")\n",
    "    statistics.to_csv(current_statistics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, statistics in zip(model_names, models_statistics_frames):\n",
    "    statistics_description = statistics.describe()\n",
    "    statistics_description_path = os.path.join(statistics_path, f\"{model_name}_description.csv\")\n",
    "    statistics_description.to_csv(statistics_description_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_accuracy_statistics = [frame.loc[frame.loc[:, \"accuracy\"].idxmax()] for frame in models_statistics_with_cut_off_points_frames]\n",
    "highest_accuracy_statistics = [pd.Series(statistics, name=name) for name, statistics in zip(model_names, highest_accuracy_statistics)]\n",
    "highest_accuracy_statistics = pd.concat(highest_accuracy_statistics, axis=1)\n",
    "\n",
    "lowest_mae_statistics = [frame.loc[frame.loc[:, \"mae\"].idxmin()] for frame in models_statistics_with_cut_off_points_frames]\n",
    "lowest_mae_statistics = [pd.Series(statistics, name=name) for name, statistics in zip(model_names, lowest_mae_statistics)]\n",
    "lowest_mae_statistics = pd.concat(lowest_mae_statistics, axis=1)\n",
    "\n",
    "lowest_mse_statistics = [frame.loc[frame.loc[:, \"mse\"].idxmin()] for frame in models_statistics_with_cut_off_points_frames]\n",
    "lowest_mse_statistics = [pd.Series(statistics, name=name) for name, statistics in zip(model_names, lowest_mse_statistics)]\n",
    "lowest_mse_statistics = pd.concat(lowest_mse_statistics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_accuracy_statistics_path = os.path.join(statistics_path, f\"highest_accuracy_statistics.csv\")\n",
    "highest_accuracy_statistics.to_csv(highest_accuracy_statistics_path)\n",
    "\n",
    "lowest_mae_statistics_path = os.path.join(statistics_path, f\"lowest_mae_statistics.csv\")\n",
    "lowest_mae_statistics.to_csv(lowest_mae_statistics_path)\n",
    "\n",
    "lowest_mse_statistics_path = os.path.join(statistics_path, f\"lowest_mse_statistics.csv\")\n",
    "lowest_mse_statistics.to_csv(lowest_mse_statistics_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crowd count without cut-off-point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_crowd(probabilities_for_each_image: np.ndarray) -> Iterable[np.ndarray]:\n",
    "    crowd_counts = np.sum(probabilities_for_each_image, axis=1)\n",
    "    crowd_counts = np.around(crowd_counts)\n",
    "    yield crowd_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_counts_generators: Iterable[Iterable[np.ndarray]] = (count_crowd(probabilities) for probabilities in crowd_counts_probabilities)\n",
    "\n",
    "counts_differences_generators: Iterable[Iterable[np.ndarray]] = ((crowd_counts - count_test for crowd_counts in generator) for generator in crowd_counts_generators)\n",
    "models_statistics: List[List[Dict[str, float]]] = [[calculate_statistics(difference) for difference in generator] for generator in counts_differences_generators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST = 0\n",
    "\n",
    "models_statistics: List[Dict[str, List[float]]] = [groupby_keys(statistics) for statistics in models_statistics]\n",
    "\n",
    "flatten_models_statistics: List[Dict[str, float]] = [{key: value[FIRST] for key, value in statistics.items()} for statistics in models_statistics]\n",
    "flatten_models_statistics = groupby_keys(flatten_models_statistics)\n",
    "\n",
    "models_statistics_frames = pd.DataFrame(flatten_models_statistics, index=model_names)\n",
    "models_statistics_frames = models_statistics_frames.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_statistics_path = os.path.join(statistics_path, f\"identity_filler.csv\")\n",
    "models_statistics_frames.to_csv(models_statistics_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crowd count with linear filler between double cut-off-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_CATEGORY_FILLER = 0\n",
    "UPPER_CATEGORY_FILLER = 1\n",
    "\n",
    "def count_crowd(probabilities_for_each_image: np.ndarray, cut_off_points: List[Tuple[int, int]]) -> Iterable[np.ndarray]:\n",
    "    for lower, upper in cut_off_points:\n",
    "        yield count_crowd_with_lower_and_upper_cut_off_points(probabilities_for_each_image, lower, upper)\n",
    "\n",
    "def count_crowd_with_lower_and_upper_cut_off_points(probabilities_for_each_image: np.ndarray, lower: float, upper: float) -> np.ndarray:    \n",
    "    crowd_counts: np.ndarray = get_crowd_count_matrix_for_lower_and_upper_cut_off_points(probabilities_for_each_image, lower, upper)\n",
    "    crowd_counts = np.sum(crowd_counts, axis=1)\n",
    "    \n",
    "    return crowd_counts\n",
    "\n",
    "def get_crowd_count_matrix_for_lower_and_upper_cut_off_points(probabilities_for_each_image: np.ndarray, lower: float, upper: float) -> np.ndarray:\n",
    "    lower_and_upper_category_indices = probabilities_for_each_image > lower\n",
    "    crowd_counts = np.where(lower_and_upper_category_indices, probabilities_for_each_image, LOWER_CATEGORY_FILLER)\n",
    "    \n",
    "    upper_category_indices = lower_and_upper_category_indices & (probabilities_for_each_image > upper)\n",
    "    crowd_counts = np.where(upper_category_indices, UPPER_CATEGORY_FILLER, crowd_counts)\n",
    "    \n",
    "    return crowd_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_counts_generators: Iterable[Iterable[np.ndarray]] = (count_crowd(probabilities, cut_off_points) for probabilities in crowd_counts_probabilities)\n",
    "\n",
    "counts_differences_generators: Iterable[Iterable[np.ndarray]] = ((crowd_counts - count_test for crowd_counts in generator) for generator in crowd_counts_generators)\n",
    "models_statistics: List[List[Dict[str, float]]] = [[calculate_statistics(difference) for difference in generator] for generator in counts_differences_generators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_statistics: List[Dict[str, List[float]]] = [groupby_keys(statistics) for statistics in models_statistics]\n",
    "models_statistics_frames = [pd.DataFrame(statistics) for statistics in models_statistics]\n",
    "cut_off_points_frame = pd.DataFrame(cut_off_points, columns=[\"lower\", \"upper\"])\n",
    "\n",
    "models_statistics_with_cut_off_points_frames = [concatenate(frame, cut_off_points_frame) for frame in models_statistics_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, statistics in zip(model_names, models_statistics_with_cut_off_points_frames):\n",
    "    current_statistics_path = os.path.join(statistics_path, f\"linear_filler_{model_name}.csv\")\n",
    "    statistics.to_csv(current_statistics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, statistics in zip(model_names, models_statistics_frames):\n",
    "    statistics_description = statistics.describe()\n",
    "    statistics_description_path = os.path.join(statistics_path, f\"linear_filler_{model_name}_description.csv\")\n",
    "    statistics_description.to_csv(statistics_description_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_accuracy_statistics = [frame.loc[frame.loc[:, \"accuracy\"].idxmax()] for frame in models_statistics_with_cut_off_points_frames]\n",
    "highest_accuracy_statistics = [pd.Series(statistics, name=name) for name, statistics in zip(model_names, highest_accuracy_statistics)]\n",
    "highest_accuracy_statistics = pd.concat(highest_accuracy_statistics, axis=1)\n",
    "\n",
    "lowest_mae_statistics = [frame.loc[frame.loc[:, \"mae\"].idxmin()] for frame in models_statistics_with_cut_off_points_frames]\n",
    "lowest_mae_statistics = [pd.Series(statistics, name=name) for name, statistics in zip(model_names, lowest_mae_statistics)]\n",
    "lowest_mae_statistics = pd.concat(lowest_mae_statistics, axis=1)\n",
    "\n",
    "lowest_mse_statistics = [frame.loc[frame.loc[:, \"mse\"].idxmin()] for frame in models_statistics_with_cut_off_points_frames]\n",
    "lowest_mse_statistics = [pd.Series(statistics, name=name) for name, statistics in zip(model_names, lowest_mse_statistics)]\n",
    "lowest_mse_statistics = pd.concat(lowest_mse_statistics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_accuracy_statistics_path = os.path.join(statistics_path, f\"linear_filler_highest_accuracy_statistics.csv\")\n",
    "highest_accuracy_statistics.to_csv(highest_accuracy_statistics_path)\n",
    "\n",
    "lowest_mae_statistics_path = os.path.join(statistics_path, f\"linear_filler_lowest_mae_statistics.csv\")\n",
    "lowest_mae_statistics.to_csv(lowest_mae_statistics_path)\n",
    "\n",
    "lowest_mse_statistics_path = os.path.join(statistics_path, f\"linear_filler_lowest_mse_statistics.csv\")\n",
    "lowest_mse_statistics.to_csv(lowest_mse_statistics_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
